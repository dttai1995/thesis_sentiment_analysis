{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 11:45:58 | INFO | fairseq.file_utils | loading archive file PhoBERT_base_fairseq\n",
      "2022-06-04 11:45:58 | INFO | fairseq.tasks.masked_lm | dictionary: 64000 types\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "# Load PhoBERT-base in fairseq\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "\n",
    "phobert_base_folder = os.path.join(\"PhoBERT_base_fairseq\")\n",
    "bpe_codes_file = os.path.join(phobert_base_folder , 'bpe.codes')\n",
    "phobert = RobertaModel.from_pretrained(phobert_base_folder, checkpoint_file='model.pt', bpe='fastbpe', bpe_codes=bpe_codes_file).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n",
    "subwords = phobert.encode(line)\n",
    "\n",
    "last_layer_features = phobert.extract_features(subwords)\n",
    "assert last_layer_features.size() == torch.Size([1, 9, 768])\n",
    "\n",
    "\n",
    "all_layers = phobert.extract_features(subwords, return_all_hiddens=True)\n",
    "assert len(all_layers) == 13\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tôi là sinh_viên trường Đại_học ngoại_thương .', 0.36145657300949097, 'Đại_học'), ('Tôi là sinh_viên trường ĐH ngoại_thương .', 0.291090726852417, 'ĐH'), ('Tôi là sinh_viên trường đại_học ngoại_thương .', 0.2378615140914917, 'đại_học'), ('Tôi là sinh_viên trường Kinh_tế ngoại_thương .', 0.030656956136226654, 'Kinh_tế'), ('Tôi là sinh_viên trường Quản_lý ngoại_thương .', 0.02417973428964615, 'Quản_lý')]\n"
     ]
    }
   ],
   "source": [
    "masked_line = 'Tôi là sinh_viên trường  <mask> ngoại_thương .'\n",
    "topk_filled_outputs = phobert.fill_mask(masked_line, topk=5)\n",
    "print(topk_filled_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert.eval()  # disable dropout (or leave in train mode to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens list :  tensor([    0, 11623, 31433,   453, 44334,  2080,  5922,    57,   934,  8181,\n",
      "        31686,  3078,     2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Incorporate the BPE encoder into PhoBERT\n",
    "tokens = phobert.encode('Tôn Ngộ Không phò Đường Tăng đi Tây Trúc thỉnh kinh')\n",
    "print('tokens list : ', tokens)\n",
    "# Decode ngược lại thành câu từ chuỗi index token\n",
    "phobert.decode(tokens)  # 'Hello world!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "last_layer_features = phobert.extract_features(tokens)\n",
    "assert last_layer_features.size() == torch.Size([1, 13, 768])\n",
    "assert tokens.size() == torch.Size([13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last layer features:  tensor([[[True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         ...,\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True],\n",
      "         [True, True, True,  ..., True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "all_layers = phobert.extract_features(tokens, return_all_hiddens=True)\n",
    "print('Last layer features: ', all_layers[-1] == last_layer_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from segment import WordSegmenter\n",
    "\n",
    "segmenter = WordSegmenter()\n",
    "text = 'Tôn Ngộ Không phò Đường Tăng đi thỉnh kinh tại Tây Trúc'\n",
    "words = segmenter.segment_string_to_token(text)\n",
    "for i, token in enumerate(words):\n",
    "  if token == 'phò':\n",
    "    words[i] = ' <mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây Trúc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_masked_tok = ' '.join(words)\n",
    "text_masked_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "words1 = copy(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tôn_Ngộ_Không',\n",
       " ' <mask>',\n",
       " 'Đường',\n",
       " 'Tăng',\n",
       " 'đi',\n",
       " 'thỉnh_kinh',\n",
       " 'tại',\n",
       " 'Tây',\n",
       " 'Tây_Trúc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1[8:9] = [\"Tây_Trúc\"]\n",
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây Trúc'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_masked_tok = ' '.join(words)\n",
    "text_masked_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total probability:  0.8698028372600675\n",
      "Input sequence:  Tôn_Ngộ_Không  <mask> Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Top 10 in mask: \n",
      "Tôn_Ngộ_Không và Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không đưa Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không cõng Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không cùng Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không hộ_tống Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không theo Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không dẫn Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không chở Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không , Đường Tăng đi thỉnh_kinh tại Tây Trúc\n",
      "Tôn_Ngộ_Không tháp_tùng Đường Tăng đi thỉnh_kinh tại Tây Trúc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "topk_filled_outputs = phobert.fill_mask(text_masked_tok, topk=10)\n",
    "topk_probs = [item[1] for item in topk_filled_outputs]\n",
    "print('Total probability: ', np.sum(topk_probs))\n",
    "print('Input sequence: ', text_masked_tok)\n",
    "print('Top 10 in mask: ')\n",
    "for i, output in enumerate(topk_filled_outputs):\n",
    "  print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'EnglishDefaults' has no attribute 'create_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_48132\\2445494257.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphobert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features_aligned_to_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"học_sinh cấp 3 được đến trường sau nghỉ dịch covid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages\\fairseq\\models\\roberta\\hub_interface.py\u001b[0m in \u001b[0;36mextract_features_aligned_to_words\u001b[1;34m(self, sentence, return_all_hiddens)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspacy_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malignment_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspacy_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;31m# tokenize both with GPT-2 BPE and spaCy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages\\fairseq\\models\\roberta\\alignment_utils.py\u001b[0m in \u001b[0;36mspacy_tokenizer\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mspacy_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please install spacy with: pip install spacy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'EnglishDefaults' has no attribute 'create_tokenizer'"
     ]
    }
   ],
   "source": [
    "doc = phobert.extract_features_aligned_to_words(\"học_sinh cấp 3 được đến trường sau nghỉ dịch covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.0-cp37-cp37m-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (3.7.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy) (4.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\thesis-nlp-project\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: spacy\n",
      "Successfully installed spacy-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glob2\n",
      "  Downloading glob2-0.7.tar.gz (10 kB)\n",
      "Building wheels for collected packages: glob2\n",
      "  Building wheel for glob2 (setup.py): started\n",
      "  Building wheel for glob2 (setup.py): finished with status 'done'\n",
      "  Created wheel for glob2: filename=glob2-0.7-py2.py3-none-any.whl size=9321 sha256=932d61833b627b553798a1a082d47d8c3da8385606f00cc8e626ea832a26d18f\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\d7\\3c\\72\\5300602ba1269ffce8cff5dcf7b525fee756b57455903c37ba\n",
      "Successfully built glob2\n",
      "Installing collected packages: glob2\n",
      "Successfully installed glob2-0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob2\n",
    "\n",
    "data_path = r'C:\\Users\\admin\\Documents\\code\\VNTC\\Data\\10Topics\\Ver1.1'\n",
    "train_path = os.path.join(data_path, 'Train_Full', '*', '*.txt')\n",
    "test_path = os.path.join(data_path, 'Test_Full', '*', '*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33759/33759 [01:37<00:00, 345.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def read_text(path):\n",
    "  with open(path, 'r', encoding='utf-16') as f:\n",
    "    data = f.read()\n",
    "  return data\n",
    "\n",
    "def make_data(path):\n",
    "  texts = []\n",
    "  labels = []\n",
    "  for file_path in tqdm(glob2.glob(path)):\n",
    "      try:\n",
    "          content = read_text(file_path)\n",
    "          label = file_path.split(os.sep)[-2]\n",
    "          texts.append(content)\n",
    "          labels.append(label)\n",
    "      except:\n",
    "          continue\n",
    "  return texts, labels\n",
    "\n",
    "text_train, label_train = make_data(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 50373/50373 [02:31<00:00, 332.08it/s]\n"
     ]
    }
   ],
   "source": [
    "text_test, label_test = make_data(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def _save_pkl(path, obj):\n",
    "  with open(path, 'wb') as f:\n",
    "    pickle.dump(obj, f)\n",
    "\n",
    "def _load_pkl(path):\n",
    "  with open(path, 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "  return obj\n",
    "\n",
    "# Lưu lại các files\n",
    "# _save_pkl('.\\\\text_train.pkl', text_train)\n",
    "# _save_pkl('.\\\\label_train.pkl', label_train)\n",
    "# _save_pkl('.\\\\text_test.pkl', text_test)\n",
    "# _save_pkl('.\\\\label_test.pkl', label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_train = _load_pkl('.\\\\text_train.pkl')\n",
    "label_train = _load_pkl('.\\\\label_train.pkl')\n",
    "text_test = _load_pkl('.\\\\text_test.pkl')\n",
    "label_test = _load_pkl('.\\\\label_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' Thành lập dự án POLICY phòng chống HIV/AIDS ở VN (NLĐ)- Quỹ hỗ trợ khẩn cấp về AIDS của Hoa Kỳ vừa thành lập dự án POLICY tại VN với cam kết hỗ trợ Chính phủ và nhân dân VN đối phó HIV/AIDS.Dự án có nhiệm vụ chính là cải thiện công tác phòng chống HIV/AIDS thông qua các lĩnh vực xây dựng chính sách, rà soát các văn bản pháp luật, xây dựng chiến lược quảng bá, xây dựng chương trình đào tạo về phòng chống HIV/AIDS, lên kế hoạch bố trí nguồn lực, huấn luyện và nghiên cứu về phương tiện truyền thông đại chúng, tổ chức các hoạt động nhằm giảm kỳ thị và phân biệt đối xử đối với người có HIV/AIDS... Theo TTXVN, dự án POLICY đặc biệt quan tâm đến công tác truyền thông phòng chống HIV/AIDS, coi đây là một biện pháp tích cực và hữu hiệu trong việc phòng chống có hiệu quả HIV/AIDS. Thời gian tới, dự án POLICY sẽ tiếp tục tổ chức các hoạt động nhằm nâng cao nhận thức cho những người có trách nhiệm với công tác chỉ đạo phòng chống HIV/AIDS.\\n\\n',\n",
       " 'Chinh tri Xa hoi')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train[0], label_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2000.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor encode: [63117  1302 61542  5958    11   915   222   537   933    39], shape: 256\n",
      "x1 tensor decode:  <s>Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19</s> <pad> <pad> <pad> <pad> <pad> <\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_sequence_length = 256\n",
    "def convert_lines(lines, vocab, bpe):\n",
    "    '''\n",
    "    :param lines:\n",
    "    :param vocab:\n",
    "    :param bpe:\n",
    "    :return:\n",
    "    '''\n",
    "    outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n",
    "    eos_id = 2\n",
    "    pad_id = 1\n",
    "    for idx, row in tqdm(enumerate(lines), total=len(lines)):\n",
    "        subwords = bpe.encode('<s>' + row + '</s>')\n",
    "        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
    "        if len(input_ids) > max_sequence_length:\n",
    "            input_ids = input_ids[:max_sequence_length]\n",
    "            input_ids[-1] = eos_id\n",
    "        else:\n",
    "            input_ids = input_ids + [pad_id, ] * (max_sequence_length - len(input_ids))\n",
    "        outputs[idx,: ] = np.array(input_ids, dtype=np.int32)\n",
    "    return outputs\n",
    "\n",
    "from fairseq.data import Dictionary\n",
    "\n",
    "vocab = Dictionary()\n",
    "vocab.add_from_file(os.path.join(phobert_base_folder, 'dict.txt'))\n",
    "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
    "[x1, x2] = convert_lines(lines, vocab, phobert.bpe)\n",
    "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
    "print('x1 tensor decode: ', phobert.decode(torch.tensor(x1))[:103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33759/33759 [01:32<00:00, 365.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X = convert_lines(text_train, vocab, phobert.bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33759, 256)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinh tri Xa hoi' 'Doi song' 'Khoa hoc' 'Kinh doanh' 'Phap luat'\n",
      " 'Suc khoe' 'The gioi' 'The thao' 'Van hoa' 'Vi tinh']\n",
      "Top 5 class índices [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "lb.fit(label_train)\n",
    "y = lb.fit_transform(label_train)\n",
    "print(lb.classes_)\n",
    "print('Top 5 class índices', y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X:  33759\n",
      "length of y:  33759\n"
     ]
    }
   ],
   "source": [
    "# _save_pkl(os.path.join('.', 'PhoBERT_pretrain','X1.pkl'), X)\n",
    "# _save_pkl(os.path.join('.', 'PhoBERT_pretrain','y1.pkl'), y)\n",
    "# _save_pkl(os.path.join('.', 'PhoBERT_pretrain','labelEncoder1.pkl'), lb)\n",
    "\n",
    "\n",
    "X  = _load_pkl(os.path.join('.', 'PhoBERT_pretrain','X1.pkl'))\n",
    "y  = _load_pkl(os.path.join('.', 'PhoBERT_pretrain','y1.pkl'))\n",
    "print('length of X: ', len(X))\n",
    "print('length of y: ', len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 13:15:12 | INFO | fairseq.file_utils | loading archive file PhoBERT_base_fairseq\n",
      "2022-06-04 13:15:12 | INFO | fairseq.tasks.masked_lm | dictionary: 64000 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9975, -2.3338, -2.3026, -2.2891, -2.4389, -2.4729, -2.4866, -2.3556,\n",
       "         -2.3843, -2.0854]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "\n",
    "phoBERT_cls = phobert = RobertaModel.from_pretrained(phobert_base_folder, checkpoint_file='model.pt', bpe='fastbpe', bpe_codes=bpe_codes_file)\n",
    "phoBERT_cls.eval()\n",
    "phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
    "tokens = 'Học_sinh được nghỉ học bắt đầu từ tháng 3 do ảnh hưởng của dịch covid-19'\n",
    "token_idxs = phoBERT_cls.encode(tokens)\n",
    "logprobs = phoBERT_cls.predict('new_task', token_idxs)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.5333333333333333)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(logits, targets):\n",
    "    \"\"\"\n",
    "    Đánh giá model sử dụng cho acuracy và f1-score\n",
    "    :param logits: (B,C) torch.LongTensor  giá trị predicted cho các class output\n",
    "    :param targets: (B) torch.LongTensor actual target indices\n",
    "    :return: acc\n",
    "    f1_score\n",
    "    \"\"\"\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    targets = targets.detach().cpu().numpy()\n",
    "    f1 = f1_score(targets, y_pred, average='weighted')\n",
    "    acc = accuracy_score(targets, y_pred)\n",
    "    return acc, f1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
    "                       [0.4, 0.1, 0.5],\n",
    "                       [0.1, 0.2, 0.7]]).to(device)\n",
    "targets = torch.tensor([1, 2, 2]).to(device)\n",
    "evaluate(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate(valid_loader, model, device):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in valid_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            output = model.predict('new_task', x_batch)\n",
    "            logits = torch.exp(output)\n",
    "            acc, f1 = evaluate(logits, y_batch)\n",
    "            accs.append(acc)\n",
    "            f1s.append(f1)\n",
    "    mean_acc = np.mean(accs)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "    return mean_acc, mean_f1\n",
    "\n",
    "\n",
    "def train_on_epoch(train_loader, model, optimizer, epoch, num_epochs, criteria, device, log_aggr=100):\n",
    "    model.train()\n",
    "    sum_epoch_loss = 0\n",
    "    sum_acc =0\n",
    "    sum_f1 = 0\n",
    "    start = time.time()\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model.predict('new_task', x_batch)\n",
    "        logits = torch.exp(y_pred)\n",
    "        acc, f1 = evaluate(logits, y_batch)\n",
    "        loss = criteria(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val = loss.item()\n",
    "        sum_epoch_loss += loss_val\n",
    "        sum_acc += acc\n",
    "        sum_f1 += f1\n",
    "        iter_num = epoch * len(train_loader) + i + 1\n",
    "        if i % log_aggr == 0:\n",
    "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f),  avg acc: %.4f, avg f1: %.4f, (%.2f im/s)'\n",
    "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),  sum_acc/(i+1), sum_f1/(i+1),\n",
    "                  len(x_batch) / (time.time() - start)))\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 6\n",
    "ACCUMULATION_STEPS = 5\n",
    "FOLD = 4\n",
    "LR = 0.0001\n",
    "LR_DC_STEP = 80\n",
    "LR_DC = 0.1\n",
    "CUR_DIR = os.path.dirname(os.getcwd())\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "FOLD = 4\n",
    "CKPT_PATH2 = 'model_ckpt2'\n",
    "\n",
    "if not os.path.exists(CKPT_PATH2):\n",
    "    os.mkdir(CKPT_PATH2)\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n",
    "for fold, (train_idx, val_idx) in enumerate(splits):\n",
    "    best_score = 0\n",
    "    if fold != FOLD:\n",
    "        continue\n",
    "    print(\"Training for fold {}\".format(fold))\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X[train_idx], dtype=torch.long), torch.tensor(y[train_idx], dtype=torch.long))\n",
    "    valid_dataset  = torch.utils.data.TensorDataset(torch.tensor(X[val_idx], dtype=torch.long), torch.tensor(y[val_idx], dtype=torch.long))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    MODEL_LAST_CKPT = os.path.join(CKPT_PATH2, \"last_checkpoint.pth.tar\")\n",
    "    if os.path.exists(MODEL_LAST_CKPT):\n",
    "        print('Load checkpoint model!')\n",
    "        phoBERT_cls = torch.load(MODEL_LAST_CKPT)\n",
    "    else:\n",
    "        print(\"Load model pretrained\")\n",
    "        phoBERT_cls = RobertaModel.from_pretrained(phobert_base_folder, checkpoint_file='model.pt', bpe='fastbpe', bpe_codes=bpe_codes_file).eval()\n",
    "        phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
    "        phoBERT_cls.register_classification_head('new_task', num_classes=10)\n",
    "\n",
    "    phoBERT_cls.to(DEVICE)\n",
    "    param_optimizer = list(phoBERT_cls.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    print(\"Init Optimizer\", \"LayerNorm.bias\", \"LayerNorm.Weight\")\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    num_train_optimization_steps = int(EPOCHS * len(train_dataset)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, correct_bias=False)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)\n",
    "    scheduler0 = get_constant_schedule(optimizer)\n",
    "\n",
    "    criteria = nn.NLLLoss()\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    frozen = True\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        if epoch > 0 and frozen:\n",
    "            for child in phoBERT_cls.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "            frozen = False\n",
    "            del scheduler0\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Epoch: \", epoch)\n",
    "        train_on_epoch(train_loader=train_loader,\n",
    "                       device=DEVICE,\n",
    "                       model=phoBERT_cls,\n",
    "                       criteria=criteria,\n",
    "                       optimizer=optimizer,\n",
    "                       num_epochs=EPOCHS,\n",
    "                       epoch=epoch,\n",
    "                       log_aggr=100)\n",
    "        if not frozen:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler0.step()\n",
    "        optimizer.zero_grad()\n",
    "        acc, f1 = validate(valid_loader, phoBERT_cls, device=DEVICE)\n",
    "\n",
    "        print('Epoch {} validation: acc: {:.4f}, f1: {:.4f} \\n'.format(epoch, acc, f1))\n",
    "        ckpt_dict = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': phoBERT_cls.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "        # Save model checkpoint into 'latest_checkpoint.pth.tar'\n",
    "        torch.save(ckpt_dict, MODEL_LAST_CKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
